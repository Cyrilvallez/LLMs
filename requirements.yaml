name: llm
channels:
  - nvidia # need to specify this as first channel to go grab the dependencies of pytorch-cuda correctly
  - conda-forge
  - nodefaults
dependencies:
  - python=3.11.3
  - conda
  - pip
  - pytorch::pytorch=2.2.0 # use pytorch channel
  - pytorch::pytorch-cuda=11.8 # Remove this line if your hardware does not support cuda, otherwise it will create conflicts
  - nvidia/label/cuda-11.8.0::cuda-nvcc # Remove this line if your hardware does not support cuda, otherwise it will create conflicts
  - numpy=1.24.3
  - matplotlib=3.7.1
  - seaborn=0.12.2
  - pandas=2.0.2
  - pip:
    # - transformers==4.33.1
    - git+https://github.com/Cyrilvallez/transformers.git@cache-efficiency
    - tokenizers
    - huggingface-hub
    - accelerate
    - optimum
    - gradio>=4.0.0
    - bitsandbytes
    - sentencepiece
    - protobuf
    - tiktoken
    # For flash attention 2
    - packaging
    - ninja
    # needs to be run afterwards with the flag
    # pip install flash-attn --no-build-isolation