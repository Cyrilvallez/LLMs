{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine\n",
    "import torch\n",
    "\n",
    "prompt = \"Hello everyone, I am here today to\"\n",
    "words = [' express', ' tell', ' sell']\n",
    "prompts = [prompt + word for word in words]\n",
    "\n",
    "model = engine.HFModel('bloom-560M', dtype=torch.float32)\n",
    "input_ids = model.tokenizer(prompts, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.model.forward(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.model.transformer(input_ids)\n",
    "transfo = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    foo1 = model.model.lm_head(transfo)\n",
    "    foo2 = model.model.lm_head(transfo[:, -1:, :])\n",
    "    foo3 = model.model.lm_head(transfo[:, -5:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.allclose(foo2[:, -1, :], foo3[:, -1, :], rtol=1e-5))\n",
    "torch.max(torch.abs(foo2[:, -1, :] - foo3[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.allclose(foo1[:, -1, :], foo2[:, -1, :], rtol=1e-5))\n",
    "torch.max(torch.abs(foo1[:, -1, :] - foo2[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.allclose(foo1[:, -1, :], foo3[:, -1, :], rtol=1e-5))\n",
    "torch.max(torch.abs(foo1[:, -1, :] - foo3[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rand_vector = torch.rand(3, 100, 512) * 100\n",
    "fc_layer = nn.Linear(512, 256)\n",
    "\n",
    "res1 = fc_layer(rand_vector)\n",
    "res2 = torch.stack([fc_layer(rand_vector[:, i, :]) for i in range(100)], 1)\n",
    "\n",
    "print(res1.shape, res2.shape)\n",
    "print(f'Allclose {torch.allclose(res1, res2)}')\n",
    "\n",
    "diff = torch.abs(res1 - res2)\n",
    "print(f'Diff: avg {diff.mean()}, min {diff.min()}, max {diff.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def new_forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Only compute LOGITS FOR LAST TOKEN!!!!!!!!!!!!!\n",
    "        # Greatly help to save memory!!!!!!!!!!!!!!\n",
    "        lm_logits = self.lm_head(hidden_states[:, -1:, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous().to(shift_logits.device)\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "prompt = \"Hello everyone, I am here today to\"\n",
    "\n",
    "model1 = engine.HFModel('bloom-560M', dtype=torch.float32)\n",
    "model1.forward = new_forward\n",
    "\n",
    "model2 = engine.HFModel('bloom-560M', dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = model1(prompt, num_return_sequences=5, max_new_tokens=10, seed=1)\n",
    "test2 = model2(prompt, num_return_sequences=5, max_new_tokens=10, seed=1)\n",
    "\n",
    "assert test1 == test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
